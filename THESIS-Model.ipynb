{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637bc311",
   "metadata": {},
   "source": [
    "# THESIS - Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a92bfc",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b352d54-1ecd-439f-8c74-2df2e8373c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%load_ext line_profiler\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import compute_class_weight\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('/home/ubuntu/')\n",
    "os.chdir('/home/ubuntu/')\n",
    "import src.model\n",
    "importlib.reload(src.model)\n",
    "import src.model as mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05792d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shlex\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import keras_tuner\n",
    "\n",
    "from Bio import SeqIO\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "import src\n",
    "importlib.reload(src.model)\n",
    "import src.model as mdl\n",
    "\n",
    "sys.path.append('/home/ubuntu/')\n",
    "os.chdir('/home/ubuntu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953462e2-f55a-48dd-8307-ad4d20ad35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all paths\n",
    "datapath = '/mnt/data/'\n",
    "resultspath = '/mnt/data/results/'\n",
    "predictionspath = '/mnt/data/results/all_predictions/'\n",
    "plotspath = '/mnt/data/results/plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006f771",
   "metadata": {},
   "source": [
    "# 1. Preparation of the dataset \n",
    "- One hot encoding the sequences \n",
    "- Dataset Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e05bd5-d8eb-4c9e-b712-039119476981",
   "metadata": {},
   "source": [
    "# One hot encoding\n",
    "def fasta_to_onehotencode(seq):\n",
    "    # values = list(seq)\n",
    "    # values = np.array(values)\n",
    "    base2int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    # label = int(line.strip()[1])\n",
    "\n",
    "    sequence = seq\n",
    "    # Encode sequence bases as integers, i.e. A as 0, C as 1, etc.\n",
    "    sequence_int = [base2int.get(base, 9999) for base in sequence]\n",
    "\n",
    "    sequence_onehot = tf.one_hot(sequence_int, depth=4)\n",
    "\n",
    "    return sequence_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75eabb-c399-4218-8aba-f042a99e3144",
   "metadata": {},
   "source": [
    "def fastatoarray(fasta_sequences):\n",
    "    seq_array = np.zeros((1, 400, 4))\n",
    "    for fasta in fasta_sequences:\n",
    "        name, sequence = fasta.id, str(fasta.seq)\n",
    "        new_sequence = fasta_to_onehotencode(sequence)  # onehotencode(sequence)\n",
    "        new_sequence = np.expand_dims(new_sequence, axis=0)\n",
    "        seq_array = np.vstack((seq_array, new_sequence))\n",
    "    seq_array = np.delete(seq_array, 0, 0)  # to remove the first array of zeros\n",
    "    return seq_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b44e30-010e-41ef-b179-8b2958238200",
   "metadata": {},
   "source": [
    "_starting_time = time.time()\n",
    "# change a bit this function or cite\n",
    "\n",
    "\n",
    "def tic():\n",
    "    global _starting_time\n",
    "    _starting_time = time.time()\n",
    "\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _starting_time)\n",
    "    (t_min, t_sec) = divmod(t_sec, 60)\n",
    "    (t_hour, t_min) = divmod(t_min, 60)\n",
    "    print('Time passed: {}hour:{}min:{}sec'.format(t_hour, t_min, t_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3799c3-308a-487e-9091-4f31db3abe37",
   "metadata": {},
   "source": [
    "def prepare_dataset(path):\n",
    "    current_folder = folder\n",
    "    directory2 = os.listdir(f'{path}/{folder}')\n",
    "    seq_list_positive = np.zeros((1, 400, 4))\n",
    "    seq_list_negative = np.zeros((1, 400, 4))\n",
    "    seq_list_positive_test = np.zeros((1, 400, 4))\n",
    "    seq_list_negative_test = np.zeros((1, 400, 4))\n",
    "\n",
    "    for folder2 in directory2:\n",
    "        directory3 = os.listdir(f'{path}{folder}/{folder2}')\n",
    "\n",
    "        if folder2 != \"fold-4\":  # to be split into training and validation\n",
    "            seq_list_positive_file = np.zeros((1, 400, 4))\n",
    "            seq_list_negative_file = np.zeros((1, 400, 4))\n",
    "\n",
    "            for file in directory3:\n",
    "\n",
    "                if \"fasta\" in file and 'out' not in file:\n",
    "                    fasta_sequences = SeqIO.parse(open(f'{path}{folder}/{folder2}/{file}'),\n",
    "                                                  'fasta')\n",
    "                    seq_array = fastatoarray(fasta_sequences)\n",
    "\n",
    "                    if \"positive\" in file:\n",
    "                        seq_list_positive_file = np.vstack((seq_list_positive_file,\n",
    "                                                            seq_array))\n",
    "\n",
    "                    if \"negative-1\" in file:  # 2 neg: 1 pos with 'negative' --> 1 neg : 1 pos 'negative-1'\n",
    "                        seq_list_negative_file = np.vstack((seq_list_negative_file,\n",
    "                                                            seq_array))\n",
    "\n",
    "            seq_list_negative_file = np.delete(seq_list_negative_file, 0, 0)\n",
    "            seq_list_positive_file = np.delete(seq_list_positive_file, 0, 0)\n",
    "\n",
    "            seq_list_positive = np.vstack((seq_list_positive,\n",
    "                                           seq_list_positive_file))\n",
    "            seq_list_negative = np.vstack((seq_list_negative,\n",
    "                                           seq_list_negative_file))\n",
    "\n",
    "        if folder2 == \"fold-4\":  # this folder will be used for testing\n",
    "\n",
    "            seq_list_positive_test_file = np.zeros((1, 400, 4))\n",
    "            seq_list_negative_test_file = np.zeros((1, 400, 4))\n",
    "\n",
    "            for file in directory3:\n",
    "\n",
    "                if \"fasta\" in file and 'out' not in file:\n",
    "                    fasta_sequences = SeqIO.parse(open(f'{path}/{folder}/{folder2}/{file}'),\n",
    "                                                  'fasta')\n",
    "                    seq_array = fastatoarray(fasta_sequences)\n",
    "\n",
    "                    if \"positive\" in file:\n",
    "                        seq_list_positive_test_file = np.vstack((seq_list_positive_test_file,\n",
    "                                                                 seq_array))\n",
    "\n",
    "                    if \"negative-1\" in file:\n",
    "                        seq_list_negative_test_file = np.vstack((seq_list_negative_test_file,\n",
    "                                                                 seq_array))\n",
    "\n",
    "                seq_list_negative_test_file = np.delete(seq_list_negative_test_file,\n",
    "                                                        0, 0)\n",
    "                seq_list_positive_test_file = np.delete(seq_list_positive_test_file,\n",
    "                                                        0, 0)\n",
    "\n",
    "                seq_list_positive_test = np.vstack((seq_list_positive_test,\n",
    "                                                    seq_list_positive_test_file))\n",
    "                seq_list_negative_test = np.vstack((seq_list_negative_test,\n",
    "                                                    seq_list_negative_test_file))\n",
    "\n",
    "                seq_list_negative_test = np.delete(seq_list_negative_test,\n",
    "                                                   0, 0)\n",
    "                seq_list_positive_test = np.delete(seq_list_positive_test,\n",
    "                                                   0, 0)\n",
    "\n",
    "    seq_list_negative = np.delete(seq_list_negative,\n",
    "                                  0, 0)\n",
    "    seq_list_positive = np.delete(seq_list_positive,\n",
    "                                  0, 0)\n",
    "\n",
    "    # Preparation of the labels\n",
    "    labels_positive = np.ones((np.shape(seq_list_positive)[0], 1))\n",
    "    labels_negative = np.zeros((np.shape(seq_list_negative)[0], 1))\n",
    "    labels_positive_test = np.ones((np.shape(seq_list_positive_test)[0], 1))\n",
    "    labels_negative_test = np.zeros((np.shape(seq_list_negative_test)[0], 1))\n",
    "    print('Shape of labels: \\n-positive : ', np.shape(labels_negative),\n",
    "          '\\n-negative : ', np.shape(labels_negative),\n",
    "          '\\n-positive validation : ', np.shape(labels_positive_test),\n",
    "          '\\n-negative validation : ', np.shape(labels_negative_test))\n",
    "\n",
    "    # Merging datasets\n",
    "    x = np.vstack((seq_list_positive, seq_list_negative))\n",
    "    x_test = np.vstack((seq_list_positive_test, seq_list_negative_test))\n",
    "    y = np.vstack((labels_positive, labels_negative))\n",
    "    y_test = np.vstack((labels_positive_test, labels_negative_test))\n",
    "\n",
    "    # Splitting dataset  : test, train and validation sets\n",
    "    test_size = 0.2\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y,\n",
    "                                                      test_size=test_size,\n",
    "                                                      shuffle=True)\n",
    "    x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "\n",
    "    y_train = y_train.astype(\"float32\")  # actually useful?\n",
    "    y_val = y_val.astype(\"float32\")\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_val = x_val.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "\n",
    "    print('Shape of datasets: \\n-training set : ', np.shape(x_train),\n",
    "          '\\n-validation set : ', np.shape(x_val),\n",
    "          '\\n-testing set : ', np.shape(x_test))\n",
    "\n",
    "    # Checking class ditribution in the whole dataset and training set\n",
    "\n",
    "    print('Label frequencies among the dataset')\n",
    "    plt.hist(y)\n",
    "    plt.xticks(range(2))\n",
    "    plt.title('Label Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(y_train)\n",
    "    plt.xticks(range(2))\n",
    "    plt.title('Label Frequency training set')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(y_val)\n",
    "    plt.xticks(range(2))\n",
    "    plt.title('Label Frequency validation set')\n",
    "    plt.show()\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89762bc1",
   "metadata": {},
   "source": [
    "# 2. Standard Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc4e6e",
   "metadata": {},
   "source": [
    "# 2.1 Create the baseline model \n",
    "- Initial model architecture inspired by pysster (citation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccff62b-3c0b-4afa-95d9-6810c4f5815b",
   "metadata": {},
   "source": [
    "def create_baseline():\n",
    "\n",
    "    METRICS = [\n",
    "        keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalseNegatives(name='fn'), \n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(curve='ROC', name='auroc'),\n",
    "        keras.metrics.AUC(curve='PR', name='auprc')\n",
    "    ]\n",
    "\n",
    "    # Parameters \n",
    "    input_shape = (400, 4)\n",
    "    filters = 30\n",
    "    kernel_size = 25\n",
    "    pool_size = 2\n",
    "    strides = 2\n",
    "    loss = 'binary_crossentropy'\n",
    "    optimizer = 'adam'\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    model.add(keras.layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              kernel_initializer='random_normal',\n",
    "              activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=pool_size, strides=strides,\n",
    "                                        padding='valid'))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "              kernel_initializer='random_normal',\n",
    "              activation='relu', input_shape=input_shape ))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=pool_size, strides=strides,\n",
    "                                        padding='valid'))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.6))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.6))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49ecdd-7da4-4ca5-8e85-cce6358ac847",
   "metadata": {},
   "source": [
    "# Summary of the model\n",
    "model = create_baseline()\n",
    "\n",
    "print(model.layers)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2514d86",
   "metadata": {},
   "source": [
    "# 2.2 Fitting and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58f234-c22a-4f85-ad81-acc927f0e7c4",
   "metadata": {},
   "source": [
    "print(\"Fit model on training data\")\n",
    "\n",
    "\n",
    "def fit_standard(x_train, y_train, x_val, y_val, folder):\n",
    "\n",
    "    model = create_baseline()\n",
    "\n",
    "    # Parameters\n",
    "    epochs = 50\n",
    "    my_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5),  # introducing EarlyStopping to avoid overfitting\n",
    "        #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=my_callbacks)\n",
    "\n",
    "    # list all data in history\n",
    "\n",
    "    # Summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # Summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Summarize history for auroc \n",
    "    plt.plot(history.history['auroc'])\n",
    "    plt.plot(history.history['val_auroc'])\n",
    "    plt.plot(history.history['auprc'])\n",
    "    plt.plot(history.history['val_auprc'])\n",
    "    plt.title('model auroc and auprc')\n",
    "    plt.ylabel('auroc and auprc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['auroc_train', 'auroc_test',\n",
    "                'auprc_train', 'auprc_test'],\n",
    "               loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    model.save(f'{resultspath}best_models/standard_model_%s' % current_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36651927",
   "metadata": {},
   "source": [
    "# 2.3 Testing and saving the Standard model\n",
    "- Testing on the same folder on which later the architecture is going to be tuned, for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d5569-58f4-460a-994d-fad92d42d683",
   "metadata": {
    "tags": []
   },
   "source": [
    "def test_standard(model_path, x_test, y_test, folder):\n",
    "    model = keras.models.load_model(model_path)\n",
    "\n",
    "    # Testing and saving the predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    '''def plot_PRC(y_test, y_pred): \n",
    "        plot = PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
    "        pl= plot.ax_.set_title(\"Precision-Recall curve\")\n",
    "        return pl'''\n",
    "    # Precision Recall Curve\n",
    "    plt = mdl.plot_PRC(y_test, y_pred)\n",
    "    plt.savefig(f'{plotspath}PRC{current_folder}_standard.png',\n",
    "                bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    '''def plot_ROC(y_test, y_pred):\n",
    "\n",
    "        false_pr, true_pr, thresholds = metrics.roc_curve(y_test, y_pred, drop_intermediate=False)\n",
    "\n",
    "        # Generate figure\n",
    "\n",
    "        fig = pl.figure(figsize=(14,7))\n",
    "        ax = fig.add_subplot(121)\n",
    "\n",
    "        pl.plot(false_pr, true_pr, label=\" (AUC=%.2f)\" % metrics.roc_auc_score(y_test, y_pred))\n",
    "        ax.plot([0,1], [0,1], color=\"grey\",label=\"Random Classifier\",linestyle=\"--\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.grid(color=\"#CCCCCC\")\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        pl.legend()\n",
    "        pl.tight_layout()\n",
    "        return pl\n",
    "    '''\n",
    "    # Receiver Operating Characteristics \n",
    "    plt = mdl.plot_ROC(y_test, y_pred)\n",
    "    plt.savefig(f'{plotspath}ROC{current_folder}_standard.png',\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de944fe",
   "metadata": {},
   "source": [
    "# 3. Finding the best model via Random Search\n",
    "- Identification of the optimal parameters\n",
    "- Saving of the model after its evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752c8bd-31cd-48eb-be37-a8e06c4b09e6",
   "metadata": {},
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(curve='ROC', name='auroc'),\n",
    "      keras.metrics.AUC(curve='PR', name='auprc')\n",
    "    ]\n",
    "\n",
    "    input_shape = (400, 4)\n",
    "\n",
    "    # Hyperparameter search\n",
    "\n",
    "    hp_filters = hp.Int('filters', min_value=10, max_value=60, step=10)\n",
    "    hp_kernel_size = hp.Int('kernel_size', min_value=10, max_value=60, step=5)\n",
    "    hp_pool_size = hp.Int('pool_size', min_value=1, max_value=10, step=1)\n",
    "    hp_strides = hp.Int('strides', min_value=1, max_value=10, step=1)\n",
    "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2,\n",
    "                                sampling=\"log\")\n",
    "    # hp_padding = hp.Choice('padding', ['valid','same'])\n",
    "    hp_padding = 'same'\n",
    "    hp_kernel_initializer = hp.Choice('initializer',\n",
    "                                      ['random_normal', 'random_uniform'])\n",
    "    hp_activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "\n",
    "    # Create model\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    model.add(keras.layers.Conv1D(filters=hp_filters, kernel_size=hp_kernel_size,\n",
    "              kernel_initializer=hp_kernel_initializer,\n",
    "              activation=hp_activation, input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=hp_pool_size,\n",
    "                                        strides=hp_strides, padding=hp_padding))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Conv1D(filters=hp_filters,\n",
    "                                  kernel_size=hp_kernel_size,\n",
    "                                  kernel_initializer=hp_kernel_initializer,\n",
    "                                  activation=hp_activation,\n",
    "                                  input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=hp_pool_size,\n",
    "                                        strides=hp_strides, padding=hp_padding))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Flatten())  # without parameters\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=1024, step=32),\n",
    "                activation=hp_activation\n",
    "                )\n",
    "        )\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(10, activation=hp_activation))\n",
    "    model.add(keras.layers.Dropout(0.6))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # many models train better when reducing the learning rate gradually -- not in this case\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                                                 # 0.001,\n",
    "                                                                 # decay_steps=50*1000,\n",
    "                                                                 # decay_rate=1,\n",
    "                                                                  #staircase=False)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(hp_learning_rate),\n",
    "                  metrics=METRICS)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7a4d2-f347-41d5-b7f1-4d540534fd0a",
   "metadata": {},
   "source": [
    "def prepare_best_model(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(curve='ROC', name='auroc'),\n",
    "      keras.metrics.AUC(curve='PR', name='auprc')\n",
    "    ]\n",
    "\n",
    "    my_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "        # restore best weights added later), #better to reduce to 2 for the search \n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "        # tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "        ]\n",
    "\n",
    "    model = build_model(keras_tuner.HyperParameters())\n",
    "    print(model.summary())\n",
    "\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        hypermodel=build_model,\n",
    "        # objective=keras_tuner.Objective(\"val_auroc\", direction=\"max\"), #val_loss , val_accuracy ...\n",
    "        # objective = keras_tuner.Objective(\"val_loss\", direction=\"min\"),\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=200,\n",
    "        executions_per_trial=1,  # probably better to change to 1 and increase the max_trials\n",
    "        overwrite=False,  # 'False' because I found the best combination\n",
    "        directory=\"my_dir\",\n",
    "        project_name=\"models\",\n",
    "        max_consecutive_failed_trials=10,\n",
    "    )\n",
    "\n",
    "    print(tuner.search_space_summary())\n",
    "    tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
    "    '''\n",
    "    # Get the top 2 models.\n",
    "    models = tuner.get_best_models(num_models=4)\n",
    "    best_model = models[0]\n",
    "    # Build the model.\n",
    "    best_model.build(input_shape=(None, 400, 4))\n",
    "    best_model.summary() #is this block relevant? '''\n",
    "    \n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(5)\n",
    "    #SAVE THESE HYPERPARAMETERS AND USE THEM FOR create_baseline_model()\n",
    "    #at this point the optimal parameters have been identified, the next part of training and prediction is the same for all models \n",
    "    \n",
    "    # Build the model with the best hp.\n",
    "    model = build_model(best_hps[0])\n",
    "    \n",
    "    #saving the best model for testing and aggregated evaluation later\n",
    "    model.save(f'{resultspath}best_models/best_hps_model') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac182155-7ec8-42ec-ad59-c4735153ee36",
   "metadata": {},
   "source": [
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d131ae-57dc-4076-b4ce-0fa83f0a77ab",
   "metadata": {},
   "source": [
    "def training_model(x_train, y_train, x_val, y_val, x_test, y_test, folder, resultspath):\n",
    "    \n",
    "    METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(curve='ROC', name='auroc'),\n",
    "      keras.metrics.AUC(curve='PR', name='auprc')  \n",
    "    ]\n",
    "    \n",
    "    my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5), #better to reduce to 2 for the search \n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    \n",
    "    # Fitting the model\n",
    "    model = keras.models.load_model(f'{resultspath}best_models/best_hps_model')\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=200,\n",
    "                        validation_data= (x_val,y_val),\n",
    "                        callbacks = my_callbacks)\n",
    "    \n",
    "    # list all data in history --> add name of the rbp!!!!\n",
    "    print(history.history.keys())\n",
    "    \n",
    "    plt.title('Best model evaluation for %s' %folder)\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy %s'  %folder)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss %s'  %folder)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for auroc \n",
    "    plt.plot(history.history['auroc'])\n",
    "    plt.plot(history.history['val_auroc'])\n",
    "    plt.plot(history.history['auprc'])\n",
    "    plt.plot(history.history['val_auprc'])\n",
    "    plt.title('model auroc and auprc %s'  %folder)\n",
    "    plt.ylabel('auroc and auprc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['auroc_train', 'auroc_test','auprc_train', 'auprc_test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    model.save(f'{resultspath}best_models/best_model_base%s' %folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89248c",
   "metadata": {},
   "source": [
    "# 3.2 Testing the model \n",
    "- Testing using different methods (Precision Recall Curve, ROC Curve, Confusion Matrix)\n",
    "- Saving the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3e6b7-279c-40bb-89bf-e5b9cab77b64",
   "metadata": {},
   "source": [
    "def testing_model(path, current_folder, x_test, y_test) : \n",
    "    #reloading the model to compute the predictions\n",
    "    reconstructed_model = keras.models.load_model(path)\n",
    "    y_pred = reconstructed_model.predict(x_test)\n",
    "    threshold = 0.5\n",
    "    y_pred_thresh = np.where(y_pred > threshold, 1,0)\n",
    "    \n",
    "    # Precision Recall Curve \n",
    "    plt = mdl.plot_PRC(y_test, y_pred)\n",
    "    plt.savefig(f'{plotspath}PRC{current_folder}_opt.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Receiver Operating Characteristics\n",
    "    plt = mdl.plot_ROC(y_test, y_pred)\n",
    "    plt.savefig(f'{plotspath}ROC{current_folder}_opt.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt= mdl.plot_CM(y_test, y_pred_thresh)\n",
    "    plt.savefig(f'{plotspath}MC{current_folder}_opt.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    '''\n",
    "    def plot_CM(y_test, y_pred):\n",
    "        matrix = confusion_matrix(y_test, y_pred)\n",
    "        names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "        counts = ['{0:0.0f}'.format(value) for value in\n",
    "                        matrix.flatten()]\n",
    "        percentages = ['{0:.2%}'.format(value) for value in\n",
    "                             matrix.flatten()/np.sum(matrix)]\n",
    "        labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "                  zip(names,counts,percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "        plot = sns.heatmap(matrix, annot=labels, fmt='', cmap='Blues')\n",
    "        return plot'''\n",
    "    # saving the predictions (0,1) in a csv file -- in probabilities \n",
    "    df = pd.DataFrame(y_pred, columns=['Predictions'])\n",
    "    df['True'] = y_test\n",
    "    df.to_csv(f\"{predictionspath}predictions/%s.csv\" % current_folder)\n",
    "    print(\"Prediction saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e75e73-b1ec-4cb4-933d-9b7cb2515082",
   "metadata": {},
   "source": [
    "# Processing one RBP\n",
    "- The chosen protein is '', which has high methylation rate and this could improve the ability of the model to discern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a4d87-75d2-4f15-b074-04552cae26ca",
   "metadata": {},
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "path = f'{datapath}processed/ENCODE-Sofia/'\n",
    "directory1 = os.listdir(path)\n",
    "\n",
    "df_methylation = pd.read_csv(f'{resultspath}methylation_rate.csv')\n",
    "df_high = df_methylation[df_methylation.methylation == 'high']\n",
    "list_high_meth = list(df_high.RBP)\n",
    "\n",
    "for folder in directory1:\n",
    "    #if folder in list_high_meth:  # to be done for high methylation RBPs\n",
    "    if folder == 'METAP2_K562':\n",
    "        try:\n",
    "            # preparation of the dataset\n",
    "            path_figure = f'{plotspath}summary_metrics/{folder}/'\n",
    "            x_train, y_train, x_val, y_val, x_test, y_test = mdl.prepare_original_dataset(path, folder) #prepares the dataset in one CV configuration\n",
    "\n",
    "            y_labels = np.reshape(y_train, (len(y_train), ))\n",
    "            class_weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                                                 classes=np.unique(y_labels),\n",
    "                                                 y=y_labels)\n",
    "            class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "            # STANDARD MODEL\n",
    "            output_folder = '_standard_original'\n",
    "\n",
    "            # fitting and testing \n",
    "            model = mdl.create_baseline_standard()\n",
    "            mdl.training_model(model, class_weights, x_train, y_train, x_val, y_val, x_test, y_test, folder, resultspath, output_folder, path_figure)\n",
    "\n",
    "            model_path = f'{resultspath}best_models/{output_folder}/%s' % folder  # path of the standard model, it has the same configuration for all RBPs \n",
    "            mdl.testing_model(model_path, folder, x_test, y_test, predictionspath, output_folder, path_figure)\n",
    "\n",
    "            # TUNED MODEL\n",
    "            output_folder = '_tuned_original'\n",
    "\n",
    "            # prepare and save the best configuration per RBP to be used later\n",
    "            mdl.prepare_best_model(x_train, y_train, x_val, y_val, x_test, y_test, resultspath, folder)  # this path is going to be used in all other tests. \n",
    "\n",
    "            # training the model on the original dataset\n",
    "            model = keras.models.load_model(f'{resultspath}best_models/best_hps_model{folder}') #4 channels \n",
    "            mdl.training_model(model, class_weights, x_train, y_train, x_val, y_val, x_test, y_test, folder, resultspath, output_folder, path_figure) # best model is not trained on any data\n",
    "\n",
    "            # evaluation and storage of the predicted labels\n",
    "            model_path = f'{resultspath}best_models/{output_folder}/%s' % folder # trained model \n",
    "            mdl.testing_model(model_path, folder, x_test, y_test, predictionspath, output_folder, path_figure)\n",
    "        except:  # I don't remember which type of error was raised \n",
    "            print(f'Exception occurred in {folder}')\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963cfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "path = f'{datapath}processed/ENCODE-Sofia/'\n",
    "directory1 = os.listdir(path)\n",
    "\n",
    "df_methylation = pd.read_csv(f'{resultspath}methylation_rate.csv')\n",
    "df_high = df_methylation[df_methylation.methylation == 'high']\n",
    "list_high_meth = list(df_high.RBP)\n",
    "\n",
    "for folder in directory1:\n",
    "    #if folder in list_high_meth:  # to be done for high methylation RBPs\n",
    "    if folder == 'PCBP1_HepG2':\n",
    "        try:\n",
    "            # preparation of the dataset\n",
    "            path_figure = f'{plotspath}summary_metrics/{folder}/'\n",
    "            x_train, y_train, x_val, y_val, x_test, y_test = mdl.prepare_original_dataset(path, folder) #prepares the dataset in one CV configuration\n",
    "            #trying with small sized model\n",
    "            x_train = x_train[:1200][:][:]\n",
    "            y_train = y_train[:1200]\n",
    "            x_val = x_val[:300][:][:]\n",
    "            y_val = y_val[:300]\n",
    "            \n",
    "            print(len(x_train))\n",
    "            print(len(x_val))\n",
    "            print(len(y_train))\n",
    "            print(len(y_val))\n",
    "            \n",
    "            y_labels = np.reshape(y_train, (len(y_train), ))\n",
    "            class_weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                                                 classes=np.unique(y_labels),\n",
    "                                                 y=y_labels)\n",
    "            class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "        \n",
    "            # STANDARD MODEL\n",
    "            output_folder = '_standard_original'\n",
    "\n",
    "            # fitting and testing \n",
    "            model = mdl.create_baseline_standard()\n",
    "            mdl.training_model(model, class_weights, x_train, y_train, x_val, y_val, x_test, y_test, folder, resultspath, output_folder, path_figure)\n",
    "\n",
    "            model_path = f'{resultspath}best_models/{output_folder}/%s' % folder  # path of the standard model, it has the same configuration for all RBPs \n",
    "            mdl.testing_model(model_path, folder, x_test, y_test, predictionspath, output_folder, path_figure)\n",
    "            \n",
    "            # ARTIFICIAL HYPERPARAMETERS\n",
    "            output_folder = '_suggested_tuned_original'\n",
    "\n",
    "            # fitting and testing \n",
    "            model = mdl.create_baseline_model((400, 4))\n",
    "            mdl.training_model(model, class_weights, x_train, y_train, x_val, y_val, x_test, y_test, folder, resultspath, output_folder, path_figure)\n",
    "\n",
    "            model_path = f'{resultspath}best_models/{output_folder}/%s' % folder  # path of the standard model, it has the same configuration for all RBPs \n",
    "            mdl.testing_model(model_path, folder, x_test, y_test, predictionspath, output_folder, path_figure)\n",
    "        except:  # I don't remember which type of error was raised \n",
    "            print(f'Exception occurred in {folder}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637247ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
